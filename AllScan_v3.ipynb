{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import Can_Algorithms as alg\n",
    "import numpy as nmp\n",
    "import logging as log\n",
    "import datetime\n",
    "import os\n",
    "import umap\n",
    "import seaborn as sns  # for visualizing the confusion matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from appconfig import Config\n",
    "from sklearn.feature_selection import RFE\n",
    "import itertools\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Config with the environment ('dev' or 'prod') and get Configuration value\n",
    "appconfig = Config('prod')\n",
    "log_folder=appconfig.getconfig_from_key(\"log_folder\")\n",
    "log_filename=appconfig.getconfig_from_key(\"log_filename\")\n",
    "datasets=appconfig.getconfig_from_key(\"datasets\")\n",
    "train_models=appconfig.getconfig_from_key(\"train_models\")\n",
    "\n",
    "# Configure logging to save log file in the folder\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "log_file = os.path.join(log_folder, log_filename)\n",
    "log.basicConfig(filename=log_file,\n",
    "                level=log.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "                label_encoder = LabelEncoder()\n",
    "                df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "#Function to convert hex string to integer\n",
    "def hex_to_int(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return int(x, 16)\n",
    "        except ValueError:\n",
    "            return nmp.nan\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def getDate():\n",
    "    current_ts = datetime.datetime.now()\n",
    "    formatted_time= current_ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return current_ts,formatted_time\n",
    "\n",
    "def PreprocessData(data, testSize=0.2): \n",
    "    ds = pd.read_csv(data, header=None)\n",
    "     # Assign column names\n",
    "    ds.columns = appconfig.getconfig_from_key(\"dataset_columns\")\n",
    "    print(\"---Dataset Info---\")\n",
    "    print(ds.info())\n",
    "    print(\"---Dataset Objects---\")\n",
    "    print(ds.describe(include='object'))\n",
    "    print(\"---Dataset Shape---\")\n",
    "    # print(ds.shape())\n",
    "    print(ds.isnull().sum())    \n",
    "\n",
    "    total = ds.shape[0]\n",
    "    missing_columns = [col for col in ds.columns if ds[col].isnull().sum() > 0]\n",
    "    for col in missing_columns:\n",
    "        null_count = ds[col].isnull().sum()\n",
    "        per = (null_count/total) * 100\n",
    "        print(f\"{col}: {null_count} ({round(per, 3)}%)\")\n",
    "\n",
    "    print(f\"Number of duplicate rows: {ds.duplicated().sum()}\")\n",
    "\n",
    "    #Plot the type of Attacks\n",
    "    sns.countplot(x=ds['Flag'])\n",
    "\n",
    "    print('Class distribution Training set:')\n",
    "    print(ds['Flag'].value_counts())\n",
    "\n",
    "    #Label encode of dataset\n",
    "    label_encode(ds)  \n",
    "\n",
    "    \n",
    "    ds.drop(['Timestamp'], axis=1, inplace=True)\n",
    "    # ds.head()\n",
    "    X=ds.drop(['Flag'],axis=1)\n",
    "    y=ds['Flag']\n",
    "\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "\n",
    "    rfe = RFE(rfc, n_features_to_select=5)\n",
    "    rfe = rfe.fit(X, y)\n",
    "\n",
    "    feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), X.columns)]\n",
    "    selected_features = [v for i, v in feature_map if i==True]\n",
    "    # selected_features=['CAN_ID', 'DATA0', 'DATA1', 'DATA2', 'DATA3']\n",
    "\n",
    "    print(\"-----------Selected features----------\")\n",
    "    print(selected_features)\n",
    "\n",
    "    X = X[selected_features] #Set the best features for training\n",
    "  \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=42)\n",
    "    # return x_train, x_test, y_train, y_test\n",
    "    scale = StandardScaler()\n",
    "    X_train = scale.fit_transform(X)\n",
    "    print(\"X Train\")\n",
    "    print(X_train)\n",
    "\n",
    "     # Impute missing values using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(x_train)\n",
    "   \n",
    "    X_test_imputed = imputer.transform(x_test)\n",
    "    return X_train_imputed,X_test_imputed,y_train,y_test\n",
    "       \n",
    "\n",
    "def PlotBarGraph(models,metrics,values,title,x_label,y_label):\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(5,3))\n",
    "\n",
    "    # Plot bars for each metric and model\n",
    "    bar_width = 0.2\n",
    "    index = nmp.arange(len(models))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.bar(index + i * bar_width, values[i], bar_width, label=metric)\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.xticks(index + bar_width * 1.5, models)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def PlotLineGraph(models,metrics,values,title,x_label,y_label):\n",
    "    # Plot curved lines for each metric and model\n",
    "    for i, metric in enumerate(metrics):\n",
    "        f = interp1d(nmp.arange(len(models)), values[i], kind='cubic')\n",
    "        x_new = nmp.linspace(0, len(models) - 1, 100)\n",
    "        y_new = f(x_new)\n",
    "        plt.plot(x_new, y_new, label=metric)\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.xticks(nmp.arange(len(models)), models)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix with specified labels for normal and attack traffic.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        title = 'Confusion matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Ensure labels are in the correct order: [Normal, Attack]\n",
    "    classes = ['Normal', 'Attack']\n",
    "\n",
    "    # Print confusion matrix values\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, ax=ax, xticklabels=classes, yticklabels=classes)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_attack_scatter(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function plots a scatter diagram of the actual vs. predicted attack/normal status.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    indices = nmp.arange(len(y_true))  # Generate indices for x-axis\n",
    "\n",
    "    # Plotting actual values\n",
    "    plt.scatter(indices, y_true, color='blue', alpha=0.5, marker='o', label='Actual')\n",
    "\n",
    "    # Plotting predicted values\n",
    "    plt.scatter(indices, y_pred, color='red', alpha=0.5, marker='x', label='Predicted')\n",
    "\n",
    "    plt.title('Scatter Plot of Actual vs. Predicted Attacks/Normal Traffic')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Attack Status (1 for Attack, 0 for Normal)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_tsne(X, y_true, y_pred, perplexity=30, n_components=2, learning_rate=200):\n",
    "    \"\"\"\n",
    "    Function to plot t-SNE with actual and predicted labels.\n",
    "    Args:\n",
    "    - X: Feature matrix.\n",
    "    - y_true: Actual labels.\n",
    "    - y_pred: Predicted labels.\n",
    "    - perplexity: t-SNE perplexity parameter.\n",
    "    - n_components: Number of dimensions t-SNE should reduce to.\n",
    "    - learning_rate: t-SNE learning rate.\n",
    "    \"\"\"\n",
    "    # Initialize t-SNE\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, random_state=42)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot actual labels\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for cls in nmp.unique(y_true):\n",
    "        plt.scatter(X_tsne[y_true == cls, 0], X_tsne[y_true == cls, 1], label=f'Actual {cls}', alpha=0.5)\n",
    "    plt.title('t-SNE based on Actual Labels')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot predicted labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for cls in nmp.unique(y_pred):\n",
    "        plt.scatter(X_tsne[y_pred == cls, 0], X_tsne[y_pred == cls, 1], label=f'Predicted {cls}', alpha=0.5)\n",
    "    plt.title('t-SNE based on Predicted Labels')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_umap(X, y_true, y_pred, n_neighbors=15, min_dist=0.1, n_components=2):\n",
    "    \"\"\"\n",
    "    Function to plot UMAP with actual and predicted labels.\n",
    "    Args:\n",
    "    - X: Feature matrix.\n",
    "    - y_true: Actual labels.\n",
    "    - y_pred: Predicted labels.\n",
    "    - n_neighbors: The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n",
    "    - min_dist: The minimum distance apart that points are allowed to be in the low-dimensional representation.\n",
    "    - n_components: Number of dimensions UMAP should reduce to.\n",
    "    \"\"\"\n",
    "    # Initialize UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=42)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot actual labels\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for cls in nmp.unique(y_true):\n",
    "        plt.scatter(X_umap[y_true == cls, 0], X_umap[y_true == cls, 1], label=f'Actual {cls}', alpha=0.5)\n",
    "    plt.title('UMAP based on Actual Labels')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot predicted labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for cls in nmp.unique(y_pred):\n",
    "        plt.scatter(X_umap[y_pred == cls, 0], X_umap[y_pred == cls, 1], label=f'Predicted {cls}', alpha=0.5)\n",
    "    plt.title('UMAP based on Predicted Labels')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath=\"\"\n",
    "for ds_name in datasets:\n",
    "    log.info(\"#####################################\")\n",
    "    log.info(\"Running dataset: [%s]\",ds_name)\n",
    "    log.info(\"#####################################\")\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "    results =[]\n",
    "    data=[]\n",
    "    metrics=[]\n",
    "    values=[]\n",
    "\n",
    "    dataPath=appconfig.getconfig_from_keys([\"data_path\",ds_name])  \n",
    "    print(f\"Datapath: {dataPath}\")\n",
    "    #Load Data\n",
    "    x_train, x_test, y_train, y_test = PreprocessData(dataPath,0.2)\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    print(f\"x train\")\n",
    "\n",
    "    print(x_train)\n",
    "\n",
    "    total_samples = len(y_test)\n",
    "    print(f\"Total Samples: {total_samples}\")\n",
    "\n",
    "\n",
    "    # Run and evaluate each model\n",
    "    for model_name in train_models:\n",
    "        print('-------------------------------')\n",
    "        print('Model: ',model_name)\n",
    "        print('-------------------------------')        \n",
    "        if model_name == 'SVC':\n",
    "            start_ts,formatted_start_ts = getDate()\n",
    "            log.info(\"Starting SVC model...[%s]\",formatted_start_ts)\n",
    "            y_pred,training_score,testing_score = alg.SVC_Scan(x_train, y_train, x_test,y_test)\n",
    "            end_ts,formatted_end_ts = getDate()\n",
    "            data.append([\"Support Vector Classifier\", training_score, testing_score])\n",
    "            log.info('SVC model finished. Elapsed time: %s',\n",
    "            end_ts - start_ts)    \n",
    "        elif model_name == 'LOGR':\n",
    "            start_ts,formatted_start_ts = getDate()\n",
    "            log.info(\"Starting Logistic Regression model...[%s]\",formatted_start_ts)\n",
    "            y_pred,training_score,testing_score = alg.Logistic_regression_Scan(x_train, y_train, x_test,y_test) \n",
    "            end_ts,formatted_end_ts = getDate()\n",
    "            data.append([\"Logistic Regression\", training_score, testing_score])\n",
    "            log.info('Logistic Regression model finished. Elapsed time: %s',\n",
    "            end_ts - start_ts)\n",
    "        elif model_name == 'MLP':\n",
    "            start_ts,formatted_start_ts = getDate()\n",
    "            log.info(\"Starting MLP model...[%s]\",formatted_start_ts)\n",
    "            y_pred,training_score,testing_score = alg.MLP_Scan(x_train, y_train, x_test,y_test)\n",
    "            end_ts,formatted_end_ts = getDate()\n",
    "            data.append([\"MLP Classifier\", training_score, testing_score])\n",
    "            log.info('MLP model finished. Elapsed time: %s',\n",
    "            end_ts - start_ts)\n",
    "        elif model_name == 'SGD':\n",
    "            start_ts,formatted_start_ts = getDate()\n",
    "            log.info(\"Starting SGD model...[%s]\",formatted_start_ts)\n",
    "            y_pred,training_score,testing_score = alg.SGD_Scan(x_train, y_train, x_test,y_test)\n",
    "            end_ts,formatted_end_ts = getDate()\n",
    "            data.append([\"SGD Classifier\", training_score, testing_score])\n",
    "            log.info('SGD model finished. Elapsed time: %s',\n",
    "                end_ts - start_ts)\n",
    "        elif model_name == 'LRG':\n",
    "            start_ts,formatted_start_ts = getDate()\n",
    "            log.info(\"Starting Linear regression model...[%s]\",formatted_start_ts)\n",
    "            y_pred,training_score,testing_score = alg.Linear_regression_Scan(x_train, y_train, x_test,y_test)          \n",
    "            end_ts,formatted_end_ts = getDate()\n",
    "            data.append([\"Linear Regression\", training_score, testing_score])\n",
    "            log.info('Linear regression model finished. Elapsed time: %s',\n",
    "            end_ts - start_ts)\n",
    "        elif model_name == 'CNN':\n",
    "            start_ts,formatted_start_ts = getDate()\n",
    "            log.info(\"Starting CNN model...[%s]\",formatted_start_ts)\n",
    "            y_pred,training_score,testing_score = alg.CNN_Scan(x_train, y_train, x_test,y_test) \n",
    "            end_ts,formatted_end_ts = getDate()\n",
    "            data.append([\"CNN model\", training_score, testing_score])\n",
    "            log.info('CNN model finished. Elapsed time: %s',\n",
    "            end_ts - start_ts)\n",
    "\n",
    "        print(f\"--Report for \"+model_name+\"--\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        precision_score, recall_score, f1_score_val, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "        precision.append(precision_score)\n",
    "        recall.append(recall_score)\n",
    "        f1_score.append(f1_score_val)\n",
    "            # support.append(1)\n",
    "\n",
    "        support_score=(pd.Series(y_pred).value_counts() / len(y_pred)).loc[0]\n",
    "        support.append(support_score)\n",
    "        # plot_confusion_matrix(y_test, y_pred, title=f'Confusion Matrix - {model_name}')\n",
    "        # plot_attack_scatter(y_test,y_pred)\n",
    "      \n",
    "        print('Precision:',precision_score)\n",
    "        print('Recall:',recall_score)\n",
    "        print('f1 score:',f1_score_val)\n",
    "        print('Support:',support_score)\n",
    "    \n",
    "    col_names = [\"Model\", \"Train Score\", \"Test Score\"]\n",
    "    print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define the metrics to plot\n",
    "    metrics = ['Precision', 'Recall', 'F1 Score', 'Support']\n",
    "    values = [precision, recall, f1_score, support]\n",
    "\n",
    "    print(values) \n",
    "    PlotBarGraph(train_models,metrics,values,'Metrics by dataset - '+ds_name,'Models','Score')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
